{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 투표 기반 학습기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앙상블 방법은 예측기가 가능한 한 서로 독립적일 때 최고의 성능을 발휘한다. 다양한 분류기를 얻는 한 가지 방법은 각기 다른 알고리즘으로 학습하는 것입니다. 이렇게 하면 매우 다른 종류의 오차를 만들 가능성이 높기 때문에 앙상블 모델의 정확도를 향상시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/base-workspace/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)), ('rf', RandomF...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf),\n",
    "                ('rf', rnd_clf),\n",
    "                ('svc', svm_clf)],\n",
    "    voting='hard'\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.872\n",
      "SVC 0.888\n",
      "VotingClassifier 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/base-workspace/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting 방식\n",
    "\n",
    "- 다수결: voting='hard'\n",
    "- 확률 평균: voting='soft'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bagging과 페이스팅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다양한 분류기를 만드는 한 가지 방법은 각기 다른 알고리즘을 사용하는 것입니다. 또 다른 방법은 같은 알고리즘을 사용하지만 훈련 세트의 서브셋을 무작위로 구성하여 분류기를 각기 다르게 학습하는 것입니다.\n",
    "\n",
    "- 훈련 세트에서 중복을 허용하여 샘플링하는 방식을 배깅이라 하며, 중복을 허용하지 않고 샘플링하는 방식을 pasting이라 한다.\n",
    "- 배깅과 페이스팅에서는 같은 훈련 샘플을 여러 개의 예측기에 걸쳐 사용할 수 있다. 하지만 배깅만이 한 예측기를 위해 같은 훈련 샘플을 여러 번 샘플링할 수 있다. 이 샘플링과 훈련 과정은 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 사이킷런의 배깅과 페이스팅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모든 예측기가 훈련을 마치면 앙상블은 모든 예측기의 예측을 모아서 새로운 샘플에 대한 예측을 만든다. 수집 함수는 전형적으로 분류일 떄는 통계적 최빈값이고 회귀에 대해서는 평균을 계산한다. 개별 예측기는 원본 훈련 세트로 훈련시키는 것보다 훨씬 크게 편향되어 있지만 수집 함수를 통과하면 편향과 분산이 모두 감소한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, n_jobs=-1\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배깅은 일반적으로 비슷한 편향에서 더 작은 분산을 만듭니다. (훈련 세트의 오차 수가 거의 비슷하지만 결정 경계는 덜 불규칙하다)\n",
    "- 부트스트래핑은 각 예측기가 학습하는 서브셋에 다양성을 증가시키므로 배깅이 페이스팅보다 편향이 조금 더 높습니다. 하지만 이는 예측기들의 상관관계를 줄이므로 앙상블의 분산을 감소시킵니다. 전반적으로 배깅이 더 나은 모델을 만들기 때문에 일반적으로 더 선호합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. oob 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배깅을 사용하면 어떤 샘플은 한 예측기를 위해 여러 번 샘플링 되고 어떤 것은 전혀 선택되지 않을 수 있습니다. BagginCLassifier는 기본갑승로 중복을 허용하여 훈련 세트의 크기 만큼 m개의 샘플을 선택합니다. 선택되지 않은 나머지 37%를 oob 샘플이라 합니다. \n",
    "\n",
    "- 예측기가 훈련되는 동안에는 oob 샘플을 사용하지 않으므로 검증 세트나 교차 검증을 사용하지 않고 oob 샘플을 사용해 평가할 수 있습니다. 앙상블의 평가는 각 예측기의 oob 평가를 평균하여 얻습니다.\n",
    "\n",
    "- 사이킷런에서 BaggingClassifer를 만들 때 oob_score=True로 지정하면 훈련이 끝나면 자동으로 oob 평가를 수행합니다. 다음 코드는 이 과정을 보여줍니다. 평가 점수 결과는 oob_score_ 변수에 저장되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8986666666666666"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),\n",
    "    n_estimators=500,\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    oob_score=True\n",
    ")\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41397849, 0.58602151],\n",
       "       [0.32954545, 0.67045455],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.08510638, 0.91489362],\n",
       "       [0.37931034, 0.62068966],\n",
       "       [0.02463054, 0.97536946],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98412698, 0.01587302],\n",
       "       [0.75543478, 0.24456522],\n",
       "       [0.        , 1.        ],\n",
       "       [0.81018519, 0.18981481],\n",
       "       [0.8556701 , 0.1443299 ],\n",
       "       [0.95833333, 0.04166667],\n",
       "       [0.06145251, 0.93854749],\n",
       "       [0.00515464, 0.99484536],\n",
       "       [0.97029703, 0.02970297],\n",
       "       [0.97687861, 0.02312139],\n",
       "       [0.99473684, 0.00526316],\n",
       "       [0.01470588, 0.98529412],\n",
       "       [0.34640523, 0.65359477],\n",
       "       [0.9127907 , 0.0872093 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97126437, 0.02873563],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99459459, 0.00540541],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00490196, 0.99509804],\n",
       "       [0.66111111, 0.33888889],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.13592233, 0.86407767],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.33898305, 0.66101695],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.22289157, 0.77710843],\n",
       "       [0.38829787, 0.61170213],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01129944, 0.98870056],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00564972, 0.99435028],\n",
       "       [0.99470899, 0.00529101],\n",
       "       [0.92857143, 0.07142857],\n",
       "       [0.96891192, 0.03108808],\n",
       "       [0.97382199, 0.02617801],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03888889, 0.96111111],\n",
       "       [0.98275862, 0.01724138],\n",
       "       [0.00512821, 0.99487179],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01724138, 0.98275862],\n",
       "       [0.98863636, 0.01136364],\n",
       "       [0.80748663, 0.19251337],\n",
       "       [0.44324324, 0.55675676],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.70984456, 0.29015544],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.89772727, 0.10227273],\n",
       "       [1.        , 0.        ],\n",
       "       [0.6       , 0.4       ],\n",
       "       [0.12322275, 0.87677725],\n",
       "       [0.61363636, 0.38636364],\n",
       "       [0.92899408, 0.07100592],\n",
       "       [0.        , 1.        ],\n",
       "       [0.17365269, 0.82634731],\n",
       "       [0.88571429, 0.11428571],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03783784, 0.96216216],\n",
       "       [0.01796407, 0.98203593],\n",
       "       [0.30409357, 0.69590643],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.85380117, 0.14619883],\n",
       "       [0.00555556, 0.99444444],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.26285714, 0.73714286],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.93292683, 0.06707317],\n",
       "       [0.80327869, 0.19672131],\n",
       "       [0.00526316, 0.99473684],\n",
       "       [1.        , 0.        ],\n",
       "       [0.27322404, 0.72677596],\n",
       "       [0.65425532, 0.34574468],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02197802, 0.97802198],\n",
       "       [0.53932584, 0.46067416],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00543478, 0.99456522],\n",
       "       [1.        , 0.        ],\n",
       "       [0.19889503, 0.80110497],\n",
       "       [0.44059406, 0.55940594],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02234637, 0.97765363],\n",
       "       [0.99470899, 0.00529101],\n",
       "       [0.2748538 , 0.7251462 ],\n",
       "       [0.90217391, 0.09782609],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.75409836, 0.24590164],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01117318, 0.98882682],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99382716, 0.00617284],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00537634, 0.99462366],\n",
       "       [0.93296089, 0.06703911],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01522843, 0.98477157],\n",
       "       [0.26060606, 0.73939394],\n",
       "       [0.96236559, 0.03763441],\n",
       "       [0.28645833, 0.71354167],\n",
       "       [0.98947368, 0.01052632],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00552486, 0.99447514],\n",
       "       [0.67096774, 0.32903226],\n",
       "       [0.3556701 , 0.6443299 ],\n",
       "       [0.38659794, 0.61340206],\n",
       "       [0.83157895, 0.16842105],\n",
       "       [0.94358974, 0.05641026],\n",
       "       [0.08743169, 0.91256831],\n",
       "       [0.77540107, 0.22459893],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03592814, 0.96407186],\n",
       "       [0.9787234 , 0.0212766 ],\n",
       "       [0.99444444, 0.00555556],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00568182, 0.99431818],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02234637, 0.97765363],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95054945, 0.04945055],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.37714286, 0.62285714],\n",
       "       [0.2970297 , 0.7029703 ],\n",
       "       [0.01069519, 0.98930481],\n",
       "       [0.        , 1.        ],\n",
       "       [0.35678392, 0.64321608],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98918919, 0.01081081],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00490196, 0.99509804],\n",
       "       [0.63020833, 0.36979167],\n",
       "       [0.94350282, 0.05649718],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98850575, 0.01149425],\n",
       "       [0.99043062, 0.00956938],\n",
       "       [0.99450549, 0.00549451],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.11891892, 0.88108108],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03608247, 0.96391753],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04545455, 0.95454545],\n",
       "       [0.99462366, 0.00537634],\n",
       "       [0.92307692, 0.07692308],\n",
       "       [0.7486631 , 0.2513369 ],\n",
       "       [0.55050505, 0.44949495],\n",
       "       [0.        , 1.        ],\n",
       "       [0.11891892, 0.88108108],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94818653, 0.05181347],\n",
       "       [0.96984925, 0.03015075],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0106383 , 0.9893617 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.4742268 , 0.5257732 ],\n",
       "       [0.82887701, 0.17112299],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01714286, 0.98285714],\n",
       "       [0.        , 1.        ],\n",
       "       [0.9516129 , 0.0483871 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.2744186 , 0.7255814 ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97191011, 0.02808989],\n",
       "       [0.8547486 , 0.1452514 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.08152174, 0.91847826],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03553299, 0.96446701],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04516129, 0.95483871],\n",
       "       [1.        , 0.        ],\n",
       "       [0.78021978, 0.21978022],\n",
       "       [0.        , 1.        ],\n",
       "       [0.90857143, 0.09142857],\n",
       "       [1.        , 0.        ],\n",
       "       [0.20625   , 0.79375   ],\n",
       "       [0.25294118, 0.74705882],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.21428571, 0.78571429],\n",
       "       [0.96354167, 0.03645833],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.48743719, 0.51256281],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.10416667, 0.89583333],\n",
       "       [0.0718232 , 0.9281768 ],\n",
       "       [0.98888889, 0.01111111],\n",
       "       [0.01648352, 0.98351648],\n",
       "       [1.        , 0.        ],\n",
       "       [0.42458101, 0.57541899],\n",
       "       [0.12244898, 0.87755102],\n",
       "       [0.5147929 , 0.4852071 ],\n",
       "       [0.60962567, 0.39037433],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.58479532, 0.41520468],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.22222222, 0.77777778],\n",
       "       [0.80110497, 0.19889503],\n",
       "       [0.07692308, 0.92307692],\n",
       "       [1.        , 0.        ],\n",
       "       [0.82539683, 0.17460317],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01030928, 0.98969072],\n",
       "       [0.08457711, 0.91542289],\n",
       "       [0.01612903, 0.98387097],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.91      , 0.09      ],\n",
       "       [0.17647059, 0.82352941],\n",
       "       [0.96842105, 0.03157895],\n",
       "       [0.00529101, 0.99470899],\n",
       "       [0.59313725, 0.40686275],\n",
       "       [0.08938547, 0.91061453],\n",
       "       [0.98136646, 0.01863354],\n",
       "       [0.84571429, 0.15428571],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96335079, 0.03664921],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.26347305, 0.73652695],\n",
       "       [0.98913043, 0.01086957],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.82022472, 0.17977528],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.75595238, 0.24404762],\n",
       "       [0.94642857, 0.05357143],\n",
       "       [1.        , 0.        ],\n",
       "       [0.65816327, 0.34183673],\n",
       "       [0.53757225, 0.46242775],\n",
       "       [0.        , 1.        ],\n",
       "       [0.89855072, 0.10144928],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.87434555, 0.12565445],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.70689655, 0.29310345],\n",
       "       [0.08235294, 0.91764706],\n",
       "       [0.4973545 , 0.5026455 ],\n",
       "       [0.24519231, 0.75480769],\n",
       "       [0.        , 1.        ],\n",
       "       [0.87562189, 0.12437811],\n",
       "       [0.82941176, 0.17058824],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97927461, 0.02072539],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01694915, 0.98305085],\n",
       "       [0.95121951, 0.04878049],\n",
       "       [0.93846154, 0.06153846],\n",
       "       [1.        , 0.        ],\n",
       "       [0.49509804, 0.50490196],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00574713, 0.99425287],\n",
       "       [0.96571429, 0.03428571],\n",
       "       [0.01075269, 0.98924731],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97340426, 0.02659574],\n",
       "       [0.        , 1.        ],\n",
       "       [0.06806283, 0.93193717],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00531915, 0.99468085],\n",
       "       [1.        , 0.        ],\n",
       "       [0.13661202, 0.86338798],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.43888889, 0.56111111],\n",
       "       [0.12865497, 0.87134503],\n",
       "       [0.24193548, 0.75806452],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98378378, 0.01621622],\n",
       "       [0.21276596, 0.78723404],\n",
       "       [0.99459459, 0.00540541],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96045198, 0.03954802],\n",
       "       [0.35672515, 0.64327485],\n",
       "       [0.99421965, 0.00578035],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98918919, 0.01081081],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01612903, 0.98387097],\n",
       "       [0.97461929, 0.02538071],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02040816, 0.97959184],\n",
       "       [0.61988304, 0.38011696]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 랜덤 패치와 랜덤 서브스페이스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BaggingClassifier는 특성 샘플링도 지원한다. max_featuers, bootstrap_features 두 매개변수로 조절된다. 작동 방식은 max_samples, bootstrap과 동일하지만 샘플이 아니고 특성에 대한 샘플링이다.\n",
    "- 훈련 특성과 샘플을 모두 샘플링하는 것을 랜덤 패치 방식이라고 합니다.\n",
    "- 훈련 샘플을 모두 사용하고 특성은 샘플링하는 것을 랜덤 서브스페이스 방식이라 합니다.\n",
    "\n",
    "특성 샘플링은 더 다양한 예측기를 만들며 편향을 늘리는 대신 분산을 낮춥니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 랜덤 포레스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 랜덤 프로세트는 일반적으로 배깅 방법을 적용한 결정 트리의 앙상블이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_rped_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 랜덤 포레스트 알고리즘은 트리의 노드를 분할할 때 전체 특성 중에서 최선의 특성을 찾는 대신 무작위로 선택한 특성 후보 중에서 최적의 특성을 찾는 식으로 무작위를 더 주입한다. 이는 결국 트리를 더욱 다양하게 만들고 편향을 손해보는 대신 분산을 낮추어 전체적으로 더 훌륭한 모델을 만들어냅니다. 다음은 BaggingClassifier를 사용해 앞의 RandomForestClassifier와 거의 유사하게 만든 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(splitter='random',\n",
    "                           max_leaf_nodes=16),\n",
    "    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 엑스트라 트리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 랜덤 포레스트에서는 트리를 만들 때 각 노드는 무작위로 특성의 서브셋을 만들어 분할에 사용한다. 트\n",
    "- 트리를 더욱 무작위로 만들기 위해 최적의 임계값을 찾는 대신 후보 특성을 사용해 무작위로 분할한 다음 그 중에서 최상의 분할을 선택한다. 이를 익시트림 랜덤 트리앙상블이라고 부른다.\n",
    "- 여기에서도 편향이 늘지만 분산이 준다. 모든 노드마다 특성마다 가장 최적의 임계값을 찾는 것이 트리 알고리즘에서 가장 시간이 많이 소요되므로 일반적인 랜덤 포레스트보다 엑스트라 트리가 훨씬 빠르다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 특성 중요도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 랜덤 포레스트는 어떤 특성을 사용한 노드가 평균적으로 불순도를 감소시키는 지 확인하여 특성의 중요돌르 측정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.10432961938769875\n",
      "sepal width (cm) 0.0249237948056056\n",
      "petal length (cm) 0.4566556087072346\n",
      "petal width (cm) 0.4140909770994616\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris['data'], iris['target'])\n",
    "\n",
    "for name, score in zip(iris['feature_names'], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 랜덤 포레스트는 특히 특성을 선택해야 할 때 어떤 특성이 중요한 지 빠르게 확인할 수 있어 매우 편리하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 부스팅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 부스팅은 약학 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법을 말합니다. 부스팅 방법의 아이디어는 앞의 모델을 보완해나가면서 일련의 예측기를 학습해나간다. 부스팅 방법에는 여러 가지가 있지만 가장 인기 있는 것은 Adaboost, Gradient Boosting이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이전 예측기를 보완하는 새로운 예측기를 만드는 방법은 이전 모델이 과소 적합했던 훈련 샘플의 가중치를 더 높이는 것이다. 이렇게 하면 새로운 예측기는 학습하기 어려운 샘플에 점점 더 맞춰지게 됩니다. 이것이 아다부스트에서 사용하는 방식이다.\n",
    "\n",
    "- 경사 하강법은 비용 함수를 최소화 하기 위해 예측기의 모델 파라미터를 조정해가는 반면 아다부스틑 점차 더 좋아지도록 앙상블에 예측기를 추가합니다.\n",
    "\n",
    "- 모든 예측기가 훈련을 마치면 이 앙상블은 배깅이나 페이스팅과 비슷한 방식으로 예측을 만듭니다. 하지만 가중치가 적용된 훈련 세트의 전반적인 정확도에 따라 예측기마다 다른 가중치가 적용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 알고리즘\n",
    "\n",
    "1) 가중치 초기화: 샘플의 가중치를 모두 1/m으로 초기화한다.\n",
    "\n",
    "2) 에러율 조정: j 번째 예측기의 가중치가 적용된 에러율\n",
    "- rj = 틀린 샘플 가중치 합 / 전체 샘플 가중치 합\n",
    "\n",
    "3) 예측기 가중치: aj = hyperparmeter * log(1-rj) / rj\n",
    "- 올바르게 분류한 모델일 수록 가중치가 높다.\n",
    "\n",
    "4) 샘플의 가중치 업데이트\n",
    "- 잘못 분류된 샘플의 가중치가 증가한다.\n",
    "- 정분류한 샘플의 경우 - w <- w\n",
    "- 오분류한 샘플의 경우 w <- w * exp(aj)\n",
    "- 이후 정규화\n",
    "\n",
    "5) Adaboost 예측\n",
    "- y = argmax(sigma(aj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "          learning_rate=0.5, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm='SAMME.R', learning_rate=0.5\n",
    ")\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 그래디언트 부스팅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아다부스트는 반복마다 샘플의 가중치를 수정했지만 그래디언트 부스팅은 잔여 오차에 새로운 예츠기를 학습한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X, y)\n",
    "\n",
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X, y2)\n",
    "\n",
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X, y3)\n",
    "\n",
    "#y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=1.0, loss='ls', max_depth=2, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=3, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래 sklearn GBRT는 위에 앙상블과 같다.\n",
    "- learning_rate가 각 트리의 기여정도를 조정한다. 이를 0.1 처럼 낮게 설정하면 앙상블을 훈련 세트에 학습시키기 위해 많은 트리가 필요하지만 일반적으로 예측의 성능은 좋아진다. 이를 축소(shrinkage)라고 부르는 규제 방법이다.\n",
    "- 최적의 트리 수를 구하기 위해서는 조기 종료 기법을 사용할 수 있다. 간단하게 구현하려면 staged_predict() 메서드를 사용한다. 이 메서드는 훈련의 각 단계에서 앙상블에 의해 만들어진 예측기를 순회하는 반복자를 반환한다. 다음의 코드는 120개의 트리로 GRBT 앙상블을 훈련시키고 최적 트리 수를 찾기 위해 각 훈련 단계에서 검증 오차를 측정한다. 마지막에 최적 트리 수를 사용해 새로운 GRBT 앙상블을 훈련한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=2, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=53, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_esimators = np.argmin(errors)\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_esimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 많은 수의 트리를 먼저 훈련시키고 최적의 수를 찾기 위해 살펴보는 대신) 실제로 훈련을 중지하는 방법으로 조기 종료를 구현할 수 있다. warm_start=True로 설정하면 사이킷런이 fit() 메서드가 호출될 때 기존 트리를 유지하고 훈련을 추가할 수 있도록 해준다. 다음 코드는 연속해서 다섯 번의 반복 동안 검증 오차가 향상되지 않으면 훈련을 멈춘다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val) \n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    \n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Regressor는 각 트리가 훈련할 떄 사용한 훈련 샘플 비율을 지정할 수 있는 subsample 매개변수도 지원한다. 예를 들어 subsample=0.25라고 하면 각 트리는 무작위로 선택된 25% 훈련 샘플로 학습된다. 이런 기법을 **확률적 그래디언트 부스팅**이라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 스태킹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앙상블에 속한 모든 예측기의 예측을 취합하는 간단한 함수(직접 투표 같은)를 사용하는 대신 취합하는 모델을 훈련한다. \n",
    "- 아래의 세 예측기는 각각 다른 값을 예측하고 마지막 예측기(블렌더 또는 메타 학습기)가 이 예측을 입력으로 받아 최종 예측을 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 블렌더를 학습시키는 일반적인 방법은 hold-out 세트를 사용하는 것이다. 먼저 훈련 세트를 두 개의 서브셋으로 나눈다. \n",
    "- 첫 번째 서브셋은 첫 번째 레이어의 예측을 훈련시키기 위해 사용된다. 그런 다음 첫 번째 레이어의 예측기를 사용해 두 번째 홀드아웃 세트에 대한 예측을 만든다. 예측기들이 훈련하는 동안 이 샘플을 전혀 못 봤기 떄문에 새로운 예측값이 만들어진다.\n",
    "- 이제 홀드 아웃 세트의 각 샘플에 대해 세 개의 예측값이 만들어진다. 타깃값은 그대로 쓰고 앞에서 예측한 값을 입력 특성으로 사용하는 새로운 훈련 세트를 만들 수 있다. (새로운 훈련 세트는 3차원이 된다). 블렌더가 새 훈련 세트로 훈련됩니다. 즉 첫 번째 레이어의 예측을 지니고 타깃값을 예측하도록 학습된다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
